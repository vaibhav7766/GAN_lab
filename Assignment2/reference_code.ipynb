{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7emzCPzBOosL"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\__init__.py:132\u001b[0m\n\u001b[0;32m    130\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 132\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Load CelebA Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root=\"./data/celeba\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define Autoencoder (AE)\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64*64*3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64*64*3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded.view(x.size(0), 3, 64, 64)\n",
    "\n",
    "# Define Variational Autoencoder (VAE)\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64*64*3, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(512, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64*64*3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = self.fc_mu(x), self.fc_logvar(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded.view(x.size(0), 3, 64, 64), mu, logvar\n",
    "\n",
    "# Define loss functions and training loops\n",
    "# Students need to implement the training functions for AE and VAE below\n",
    "\n",
    "def train_autoencoder(model, dataloader, epochs=10, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "# Students should implement train_vae function here\n",
    "\n",
    "def visualize_reconstruction(model, dataloader, is_vae=False):\n",
    "    model.eval()\n",
    "    images, _ = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        if is_vae:\n",
    "            reconstructed, _, _ = model(images)\n",
    "        else:\n",
    "            reconstructed = model(images)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(10, 4))\n",
    "    for i in range(8):\n",
    "        axes[0, i].imshow(images[i].cpu().permute(1, 2, 0) * 0.5 + 0.5)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(reconstructed[i].cpu().permute(1, 2, 0) * 0.5 + 0.5)\n",
    "        axes[1, i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Students should train and visualize the models\n",
    "\n",
    "print(\"Visualizing Autoencoder Reconstruction\")\n",
    "visualize_reconstruction(ae, dataloader)\n",
    "\n",
    "print(\"Visualizing Variational Autoencoder Reconstruction\")\n",
    "visualize_reconstruction(vae, dataloader, is_vae=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPt5Bi566wYLfXFZtOO1cqv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
